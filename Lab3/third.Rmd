# Часть первая

### Загрузка данных

```{r, echo=FALSE}
library(ggplot2)
library(memisc)
library(DescTools)
library(broom)
library(caTools)
library(lmtest)
library(dplyr)
library(readxl)
```
```{r}
mydata <- read.csv(file="abon.txt", sep='\t', encoding = "UTF-8")
str(mydata)
```

### Построение модели линейной регрессии

```{r}
set.seed(56)
split <- sample.split(mydata$Средняя.продолжительность.разговоров, SplitRatio = 0.75)
train <- subset(mydata, split == TRUE)
test <- subset(mydata, split == FALSE)
model_l <- lm(data = train, Среднемесячный.расход ~ Средняя.продолжительность.разговоров)
summary(model_l)
```

Таким образом, уравнение регрессии имеет вид:
y = -256.946 + 181.366 * x

### Коэффициенты детерминации

```{r}
summary(model_l)$r.squared
summary(model_l)$adj.r.squared
```

Коэффициенты детерминации немногим выше среднего, поэтому можно сказать о качестве подгонки выше среднего.

### Остаточная дисперсия

```{r}
summary(model_l)$sigma^2
```

### Проверка наличия линейной связи

H0 - линейная связь отсутствует
H1 - линейная связь присутствует

```{r}
qf(0.95, 1, model_l$df.residual) > 7453
```

Т.к. F расчетное больше F табличного, нулевая гипотеза отвергается.

### Различные параметры линейной регрессии

```{r}
glance(model_l)
```

### Проверка случайности остаточной компоненты

```{r}
plot(model_l$fitted.values, model_l$residuals)
qplot(x = model_l$residuals, y = model_l$fitted.values)
```

На графиках видно, что значения остатков распределены возле нуля, но большинство значений < 0.

### Проверка равенства нулю матожидания остатков

H0 - матожидание равно нулю
H1 - матожидание отличается от нуля

```{r}
mean(model_l$residuals)
```

```{r}
rse <- model_l$df.residual
a <- mean(model_l$residuals)
b <- sd(model_l$residuals, na.rm = FALSE)
n <- sqrt(rse + 1)
tm <- a / b * n
```

```{r}
tt <- qt(0.95, rse + 1)
abs(tm) < tt
```

Так как t расчетное меньше по модулю чем t табличное, то нулевая гипотеза о равенстве нулю матожидания принимается с вероятностью 0.95.

### Проверка на гетероскедастичность
H0 - гетероскедастичность отсутствует
H1 - гетероскедастичность присутствует

Проведем тест Бройша-Погана:

```{r}
bptest(model_l)
```

Так как p-value меньше 0.05, нулевая гипотеза отвергается, следовательно, гетероскедастичность присутствует.

Проведем тест Спирмена
H0 - коэффициент Спирмена незначим (отсутствует гетероскедастичность)
H1 - коэффициент Спирмена значим (присутствует гетероскедастичность)

```{r}
cor.test(train$Средняя.продолжительность.разговоров, model_l$residuals, method = "spearman")
```

Так как p-value меньше 0.05, нулевая гипотеза отвергается, следовательно, гетероскедастичность присутствует.

### Проверка автокорреляции остатков
H0 - автокорреляция остатков отсутствует (rho = 0)
H1 - наличие положительной или отрицательной автокорреляции (rho < 0 или rho > 0)

Проведем тесты Бройша-Годфри и Дарбина-Уотсона

```{r}
bgtest(model_l)
dwtest(model_l)
```

Так как значения p-value больше 0.05, нулевая гипотеза о отсутствии автокорреляции принимается.

### Проверка согласования остатков регрессии с нормальным распределением

```{r}
qqnorm(model_l$residuals)
```

```{r}
library(sm)
Z <- model_l$residuals
hist(Z)
sm.density(Z, model="Normal", xlab="Residual", ylab="Функция плотности распределения", xlim=c(-3000, 3000))
```

Рассмотрим тесты на нормальность распределения.

H0 - распределение является нормальным
H1 - распределение не является нормальным

Тест Колмогорова-Смирнова

```{r}
library(nortest)
lillie.test(Z)

```

Так как p-value меньше 0.05, нулевая гипотеза о нормальности распределения отвергается.

Тест Шапиро-Уилка

```{r}
sh <- model_l$residuals[1:4900]
shapiro.test(sh)
```

Так как p-value меньше 0.05, нулевая гипотеза о нормальности распределения отвергается.

### Прогнозирование значений по полученной модели на тестовой выборке

```{r}
test_l <- predict(model_l, test)
plot(test$Средняя.продолжительность.разговоров, test$Среднемесячный.расход)
lines(test$Средняя.продолжительность.разговоров, test_l, col = "red")
```

# Часть вторая

## Загрузка данных

```{r}
vine <- read.csv("winequality-red.csv", sep = ",", encoding = "UTF-8")
str(vine)
```

## Построение модели линейной регрессии

Разделим выборку на обучающую и тестовую
```{r}
set.seed(1821)
split <- sample.split(vine$quality, 0.75)
train <- subset(vine, split == TRUE)
test <- subset(vine, split == FALSE)

train_y <- train$quality
train_x <- train %>% select(-quality) %>% data.matrix()
test_x <- test %>% select(-quality) %>% data.matrix()
```

Построим модель линейной регрессии
```{r}
m_lm <- lm(quality ~., data = train)
summary(m_lm)
```

## Построение прогноза

```{r}
test$predict <- predict(m_lm, test)
ggplot(test, aes(x = quality, y = predict)) + geom_point() + geom_abline() +
  scale_x_continuous(limits = c(0, 10), expand = c(0, 0)) +
  scale_y_continuous(limits = c(0, 10), expand = c(0, 0))
```

## Анализ мультиколлинеарности

Вычислим значения коэффициентов увеличения дисперсии VIFj
```{r, echo=FALSE}
library(glmnet)
library(tidyverse)
```
```{r}
VIF(m_lm)
```
Мультиколлинеарность отсутствует.

## Оценка параметров методами Ридж и Lasso

### Метод Ридж

Определим подходящее значение lambda
```{r}
lambdas <- seq(0, 150, by = 0.025)
set.seed(1877)
cv_ridge <- cv.glmnet(train_x, train_y, alpha = 0, lambda = lambdas)
plot(cv_ridge)
```
```{r}
cv_ridge$lambda.min
```

Построим модель линейной регрессии с помощью метода Ридж

```{r}
set.seed(1877)
m_ridge <- glmnet(train_x, train_y, alpha = 0, lambda = cv_ridge$lambda.min)
coef(m_ridge)
```

Построим прогноз по полученной модели

```{r}
test$ridge <- predict(m_ridge, s = cv_ridge$lambda.min, newx = test_x)
```

### Метод Lasso

Определим подходящее занчение lambda
```{r}
set.seed(1886)
cv_lasso <- cv.glmnet(train_x, train_y, alpha = 1, lambda = lambdas)
plot(cv_lasso)
```
```{r}
cv_lasso$lambda.min
```

Построим модель линейной регрессии с помощью метода Lasso

```{r}
set.seed(1886)
m_lasso <- glmnet(train_x, train_y, alpha = 1, lambda = cv_lasso$lambda.min)
coef(m_lasso)
```

Построим прогноз по полученной модели

```{r}
test$lasso <- predict(m_lasso, s = cv_lasso$lambda.min, newx = test_x)
```

## Расчёт метрик

```{r}
metrics <- matrix(0, nrow = 3, ncol = 3, dimnames = list(c("МНК", "Ридж", "Lasso"), c("MAPE", "RMSE", "MAE")))
metrics["МНК", "MAPE"] <- MAPE(x = test$predict, ref = test$quality)
metrics["МНК", "RMSE"] <- RMSE(x = test$predict, ref = test$quality)
metrics["МНК", "MAE"] <- MAE(x = test$predict, ref = test$quality)
metrics["Ридж", "MAPE"] <- MAPE(x = test$ridge, ref = test$quality)
metrics["Ридж", "RMSE"] <- RMSE(x = test$ridge, ref = test$quality)
metrics["Ридж", "MAE"] <- MAE(x = test$ridge, ref = test$quality)
metrics["Lasso", "MAPE"] <- MAPE(x = test$lasso, ref = test$quality)
metrics["Lasso", "RMSE"] <- RMSE(x = test$lasso, ref = test$quality)
metrics["Lasso", "MAE"] <- MAE(x = test$lasso, ref = test$quality)
```
Результаты сравнение метрик для различных моделей представлены в таблице
```{r}
metrics
```
